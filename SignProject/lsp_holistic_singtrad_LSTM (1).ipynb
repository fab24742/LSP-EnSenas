{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports + Drive + paths"
      ],
      "metadata": {
        "id": "LIhegBQMltON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.14 opencv-python==4.10.0.84 scikit-learn tensorflowjs --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GsFvhfDsnQfw",
        "outputId": "b1f756af-fd88-4a9f-c95e-8eba595374b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires packaging>=21, but you have packaging 20.9 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 20.9 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pandas-gbq 0.30.0 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "langsmith 0.4.42 requires packaging>=23.2, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2025.10.1 requires packaging>=24.1, but you have packaging 20.9 which is incompatible.\n",
            "jupyter-server 2.14.0 requires packaging>=22.0, but you have packaging 20.9 which is incompatible.\n",
            "libpysal 4.13.0 requires packaging>=22, but you have packaging 20.9 which is incompatible.\n",
            "shap 0.50.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "sphinx 8.2.3 requires packaging>=23.0, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "langchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 20.9 which is incompatible.\n",
            "astropy 7.1.1 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 20.9 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFUJ-njylm8t",
        "outputId": "70042adc-10f8-48bc-afc2-aca1de46f63f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TF version: 2.19.0\n",
            "RAW_DIR: /content/drive/MyDrive/SignProject/raw_videos\n",
            "PROCESSED_DIR: /content/drive/MyDrive/SignProject/processed\n",
            "MODELS_DIR: /content/drive/MyDrive/SignProject/models\n"
          ]
        }
      ],
      "source": [
        "# 0. IMPORTS BÃSICOS\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ğŸ”’ Semilla global para que el entrenamiento sea estable\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# Rutas base en tu Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/SignProject\"\n",
        "RAW_DIR = os.path.join(BASE_DIR, \"raw_videos\")\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "\n",
        "# Constantes del modelo\n",
        "MAX_FRAMES = 20      # puedes subir a 40 si tus gestos son largos\n",
        "N_FEATURES = 225     # 75 puntos * (x,y,z)\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. FunciÃ³n de extracciÃ³n de landmarks (misma lÃ³gica que entrenamiento)"
      ],
      "metadata": {
        "id": "oWlHupjqnfGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_landmarks_from_results(results):\n",
        "    \"\"\"\n",
        "    Convierte los resultados de MediaPipe Holistic en un vector 1D (225,)\n",
        "    con pose (33), mano izq (21) y mano der (21).\n",
        "    Cada punto = (x, y, z) => 75 * 3 = 225 features.\n",
        "    \"\"\"\n",
        "    def get_xyz(landmarks, n_points):\n",
        "        if landmarks is None:\n",
        "            data = [[0.0, 0.0, 0.0]] * n_points\n",
        "        else:\n",
        "            data = [[lm.x, lm.y, lm.z] for lm in landmarks]\n",
        "            if len(data) < n_points:\n",
        "                # Rellenar si faltan\n",
        "                data += [[0.0, 0.0, 0.0]] * (n_points - len(data))\n",
        "            data = data[:n_points]\n",
        "        return data\n",
        "\n",
        "    pose = get_xyz(results.pose_landmarks.landmark if results.pose_landmarks else None, 33)\n",
        "    left_hand = get_xyz(results.left_hand_landmarks.landmark if results.left_hand_landmarks else None, 21)\n",
        "    right_hand = get_xyz(results.right_hand_landmarks.landmark if results.right_hand_landmarks else None, 21)\n",
        "\n",
        "    all_points = pose + left_hand + right_hand\n",
        "    return np.array(all_points, dtype=np.float32).flatten()  # (225,)\n",
        "\n",
        "\n",
        "def mirror_sequence(seq):\n",
        "    \"\"\"\n",
        "    Recibe una secuencia (T, 225) y devuelve una versiÃ³n espejada:\n",
        "      - Refleja en horizontal: x -> 1.0 - x\n",
        "      - Intercambia mano izquierda y mano derecha\n",
        "\n",
        "    Asume el orden:\n",
        "      0-32  -> pose (33 puntos)\n",
        "      33-53 -> mano izquierda (21 puntos)\n",
        "      54-74 -> mano derecha (21 puntos)\n",
        "    \"\"\"\n",
        "    if seq.ndim != 2 or seq.shape[1] != 225:\n",
        "        raise ValueError(f\"mirror_sequence espera (T, 225), recibiÃ³ {seq.shape}\")\n",
        "\n",
        "    T, D = seq.shape\n",
        "    pts = seq.reshape(T, 75, 3).copy()  # (T, 75, 3)\n",
        "\n",
        "    # 1) Espejar en eje X (horizontal)\n",
        "    pts[..., 0] = 1.0 - pts[..., 0]\n",
        "\n",
        "    # 2) Separar segmentos: pose, mano izq, mano der\n",
        "    pose   = pts[:, :33, :]      # (T, 33, 3)\n",
        "    left   = pts[:, 33:54, :]    # (T, 21, 3)\n",
        "    right  = pts[:, 54:75, :]    # (T, 21, 3)\n",
        "\n",
        "    # 3) Reconstruir como: pose + (mano derecha como izquierda) + (mano izquierda como derecha)\n",
        "    pts_mirrored = np.concatenate([pose, right, left], axis=1)  # (T, 75, 3)\n",
        "\n",
        "    # 4) Volver a (T, 225)\n",
        "    return pts_mirrored.reshape(T, 225).astype(np.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "OkhS5Kk-njWU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. ConstrucciÃ³n del dataset secuencial para LSTM**\n",
        "\n",
        "Cada video = 1 muestra\n",
        "Cada muestra = secuencia de max_frames frames, cada frame = 225 features."
      ],
      "metadata": {
        "id": "r3fJx3PynlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_or_truncate(seq, max_frames=MAX_FRAMES):\n",
        "    \"\"\"\n",
        "    Asegura que cada secuencia tenga exactamente max_frames frames.\n",
        "    - Si hay mÃ¡s frames, recorta centrado.\n",
        "    - Si hay menos, rellena con ceros al final.\n",
        "    seq: (T, 225)\n",
        "    \"\"\"\n",
        "    n = seq.shape[0]\n",
        "\n",
        "    if n == max_frames:\n",
        "        return seq.astype(np.float32)\n",
        "\n",
        "    if n > max_frames:\n",
        "        start = max(0, (n - max_frames) // 2)\n",
        "        return seq[start:start + max_frames].astype(np.float32)\n",
        "\n",
        "    # n < max_frames â†’ padding con ceros al final\n",
        "    pad_len = max_frames - n\n",
        "    pad = np.zeros((pad_len, seq.shape[1]), dtype=np.float32)\n",
        "    return np.concatenate([seq, pad], axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "def random_temporal_crop(seq, max_frames=MAX_FRAMES, jitter=10):\n",
        "    \"\"\"\n",
        "    Data augmentation temporal:\n",
        "    - Si el video es mÃ¡s largo que max_frames, recortamos una ventana\n",
        "      de max_frames pero moviendo el inicio (para que el gesto pueda\n",
        "      aparecer en distintas partes del clip).\n",
        "    - Si es mÃ¡s corto, delegamos a pad_or_truncate.\n",
        "    \"\"\"\n",
        "    n = seq.shape[0]\n",
        "\n",
        "    if n <= max_frames:\n",
        "        return pad_or_truncate(seq, max_frames=max_frames)\n",
        "\n",
        "    max_start = n - max_frames\n",
        "    center = max_start // 2\n",
        "    start_min = max(0, center - jitter)\n",
        "    start_max = min(max_start, center + jitter)\n",
        "\n",
        "    start = np.random.randint(start_min, start_max + 1)\n",
        "    return seq[start:start + max_frames].astype(np.float32)\n",
        "\n",
        "\n",
        "def build_sequence_dataset_from_raw(\n",
        "    RAW_DIR,\n",
        "    outfile=\"landmarks_sequences_v1.npz\",\n",
        "    max_frames=MAX_FRAMES,\n",
        "    samples_per_video=5,\n",
        "    jitter=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Crea un dataset secuencial:\n",
        "      X_seq -> (n_samples, max_frames, 225)\n",
        "      y_seq -> (n_samples,)\n",
        "    Cada video original genera 'samples_per_video' secuencias con\n",
        "    recortes temporales distintos, para que el gesto pueda aparecer\n",
        "    en diferentes posiciones dentro de la ventana.\n",
        "    \"\"\"\n",
        "    classes = sorted(os.listdir(RAW_DIR))\n",
        "    classes = [c for c in classes if os.path.isdir(os.path.join(RAW_DIR, c))]\n",
        "    print(\"Clases encontradas:\", classes)\n",
        "\n",
        "    label_names = classes\n",
        "    X_seq_list = []\n",
        "    y_seq_list = []\n",
        "\n",
        "    for label_idx, cls_name in enumerate(label_names):\n",
        "        class_dir = os.path.join(RAW_DIR, cls_name)\n",
        "        video_files = [\n",
        "            f for f in os.listdir(class_dir)\n",
        "            if f.lower().endswith((\".mp4\", \".mov\", \".avi\", \".mkv\"))\n",
        "        ]\n",
        "        print(f\"\\nClase '{cls_name}': {len(video_files)} videos\")\n",
        "\n",
        "        for fname in video_files:\n",
        "            video_path = os.path.join(class_dir, fname)\n",
        "            print(\"  Procesando:\", video_path)\n",
        "\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frames_feats = []\n",
        "\n",
        "            with mp_holistic.Holistic(\n",
        "                static_image_mode=False,\n",
        "                model_complexity=1,\n",
        "                enable_segmentation=False,\n",
        "                refine_face_landmarks=False,\n",
        "                min_detection_confidence=0.5,\n",
        "                min_tracking_confidence=0.5,\n",
        "            ) as holis:\n",
        "\n",
        "                while True:\n",
        "                    ret, frame = cap.read()\n",
        "                    if not ret:\n",
        "                        break\n",
        "\n",
        "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    results = holis.process(frame_rgb)\n",
        "\n",
        "                    feats = extract_landmarks_from_results(results)  # (225,)\n",
        "                    frames_feats.append(feats)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            if len(frames_feats) == 0:\n",
        "                print(\"    âš ï¸  Video sin landmarks, se omite.\")\n",
        "                continue\n",
        "\n",
        "            seq_full = np.stack(frames_feats, axis=0)  # (T, 225)\n",
        "\n",
        "            # ğŸ”¹ NUEVO: secuencia original + secuencia espejada\n",
        "            base_sequences = [seq_full, mirror_sequence(seq_full)]\n",
        "\n",
        "            # Data augmentation temporal: varias secuencias por video\n",
        "            for base in base_sequences:\n",
        "                for _ in range(samples_per_video):\n",
        "                    seq_sample = random_temporal_crop(\n",
        "                        base,\n",
        "                        max_frames=max_frames,\n",
        "                        jitter=jitter\n",
        "                    )\n",
        "                    X_seq_list.append(seq_sample)\n",
        "                    y_seq_list.append(label_idx)\n",
        "\n",
        "    X_seq = np.stack(X_seq_list, axis=0).astype(np.float32)\n",
        "    y_seq = np.array(y_seq_list, dtype=np.int64)\n",
        "    label_names = np.array(label_names, dtype=object)\n",
        "\n",
        "    print(\"\\nShape final X_seq:\", X_seq.shape)\n",
        "    print(\"Shape final y_seq:\", y_seq.shape)\n",
        "\n",
        "    out_path = os.path.join(PROCESSED_DIR, outfile)\n",
        "    np.savez(out_path, X_seq=X_seq, y_seq=y_seq, label_names=label_names)\n",
        "    print(\"âœ… Dataset guardado en:\", out_path)\n",
        "\n",
        "    return X_seq, y_seq, label_names\n",
        "\n",
        "\n",
        "# EJECUTAR ESTA CELDA PARA CREAR / ACTUALIZAR EL DATASET\n",
        "X_seq, y_seq, label_names = build_sequence_dataset_from_raw(\n",
        "    RAW_DIR,\n",
        "    outfile=\"landmarks_sequences_v2.npz\",  # version 2 nuevo nombre\n",
        "    max_frames=MAX_FRAMES,\n",
        "    samples_per_video=3,\n",
        "    jitter=12  # un poquito mayor\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFEOUzrbnn0U",
        "outputId": "8e2671ae-28a7-4ea4-e1d0-f034cfc8aa59"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases encontradas: ['Disculpa', 'Inyeccion ', 'Marron', 'Por favor', 'Resfriado', 'Tos', 'Verde']\n",
            "\n",
            "Clase 'Disculpa': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Disculpa/diculpa.mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Disculpa/disculpa (1).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Disculpa/disculpa.mp4\n",
            "\n",
            "Clase 'Inyeccion ': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Inyeccion /inyeccion (2).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Inyeccion /inyeccion (1).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Inyeccion /inyeccion.mp4\n",
            "\n",
            "Clase 'Marron': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Marron/marron.mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Marron/IMG_5869.mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Marron/Marronf.mp4\n",
            "\n",
            "Clase 'Por favor': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Por favor/porfavor (2).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Por favor/porfavor (1).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Por favor/porfavor.mp4\n",
            "\n",
            "Clase 'Resfriado': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Resfriado/resfriado (2).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Resfriado/resfriado (1).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Resfriado/resfriado.mp4\n",
            "\n",
            "Clase 'Tos': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Tos/tos (1).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Tos/TOS.mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Tos/tos.mp4\n",
            "\n",
            "Clase 'Verde': 3 videos\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Verde/verde (2).mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Verde/IMG_5865.mp4\n",
            "  Procesando: /content/drive/MyDrive/SignProject/raw_videos/Verde/Verdef.mp4\n",
            "\n",
            "Shape final X_seq: (126, 20, 225)\n",
            "Shape final y_seq: (126,)\n",
            "âœ… Dataset guardado en: /content/drive/MyDrive/SignProject/processed/landmarks_sequences_v2.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Carga de dataset secuencial + split por porcentaje"
      ],
      "metadata": {
        "id": "9nTkIboupUbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. CARGA DE DATASET SECUENCIAL, NORMALIZACIÃ“N Y SPLIT ESTRATIFICADO\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_path = os.path.join(PROCESSED_DIR, \"landmarks_sequences_v2.npz\")\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "\n",
        "# Claves del archivo .npz\n",
        "X_seq = data[\"X_seq\"].astype(np.float32)       # (n_samples, MAX_FRAMES, 225)\n",
        "y_seq = data[\"y_seq\"].astype(np.int64)         # (n_samples,)\n",
        "label_names = data[\"label_names\"].tolist()     # lista de nombres de clase\n",
        "\n",
        "print(\"X_seq:\", X_seq.shape)\n",
        "print(\"y_seq:\", y_seq.shape)\n",
        "print(\"Clases:\", label_names)\n",
        "\n",
        "# ğŸ”§ NORMALIZACIÃ“N GLOBAL (misma para train, val y luego app.py)\n",
        "feature_mean = X_seq.mean(axis=(0, 1), keepdims=True).astype(np.float32)\n",
        "feature_std = X_seq.std(axis=(0, 1), keepdims=True).astype(np.float32) + 1e-8\n",
        "\n",
        "# Guardamos para usarlos luego en app.py / realtime\n",
        "np.save(os.path.join(MODELS_DIR, \"feature_mean_v2.npy\"), feature_mean)\n",
        "np.save(os.path.join(MODELS_DIR, \"feature_std_v2.npy\"), feature_std)\n",
        "print(\"Guardados feature_mean_v2.npy y feature_std_v2.npy en MODELS_DIR\")\n",
        "\n",
        "# Aplicar normalizaciÃ³n\n",
        "X_seq = (X_seq - feature_mean) / feature_std\n",
        "\n",
        "# ğŸ”§ SPLIT ESTRATIFICADO CON SEMILLA FIJA\n",
        "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_seq,\n",
        "    y_seq,\n",
        "    test_size=0.2,        # 20% validaciÃ³n\n",
        "    stratify=y_seq,\n",
        "    random_state=SEED,    # misma semilla que arriba\n",
        ")\n",
        "\n",
        "print(\"Train seq:\", X_train_seq.shape, y_train_seq.shape)\n",
        "print(\"Val seq:\",   X_val_seq.shape,   y_val_seq.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5aW7Ret5mu",
        "outputId": "17c47dae-8f4a-42a1-ea4f-dab66f49427f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq: (126, 20, 225)\n",
            "y_seq: (126,)\n",
            "Clases: ['Disculpa', 'Inyeccion ', 'Marron', 'Por favor', 'Resfriado', 'Tos', 'Verde']\n",
            "Guardados feature_mean_v2.npy y feature_std_v2.npy en MODELS_DIR\n",
            "Train seq: (100, 20, 225) (100,)\n",
            "Val seq: (26, 20, 225) (26,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Modelo LSTM (temporal)"
      ],
      "metadata": {
        "id": "8yLjfnFvvX07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. MODELO LSTM TEMPORAL\n",
        "\n",
        "\n",
        "num_classes = len(label_names)\n",
        "max_frames = X_train_seq.shape[1]\n",
        "n_features = X_train_seq.shape[2]\n",
        "\n",
        "model_lstm = keras.Sequential([\n",
        "    layers.Input(shape=(max_frames, n_features)),\n",
        "    layers.Masking(mask_value=0.0),\n",
        "\n",
        "    # LSTM mÃ¡s pequeÃ±o + regularizaciÃ³n L2\n",
        "    layers.LSTM(\n",
        "        64,\n",
        "        return_sequences=False,\n",
        "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
        "    ),\n",
        "\n",
        "    layers.Dense(\n",
        "        64,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
        "    ),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(\n",
        "    optimizer=keras.optimizers.Adam(5e-4),  # un poco mÃ¡s suave\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_lstm.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "xHvXPlLhvcYZ",
        "outputId": "639dbc23-27c8-4035-af7a-2acd84b1b9fa"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ masking_2 (\u001b[38;5;33mMasking\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m225\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m74,240\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              â”‚           \u001b[38;5;34m455\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ masking_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,240</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,855\u001b[0m (308.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,855</span> (308.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,855\u001b[0m (308.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,855</span> (308.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 ENTRENAMIENTO\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=20,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    min_lr=1e-5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_lstm = model_lstm.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=300,\n",
        "    batch_size=4,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "val_loss, val_acc = model_lstm.evaluate(X_val_seq, y_val_seq, verbose=0)\n",
        "print(\"\\nVAL ACCURACY LSTM:\", val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cVa2Jt8vts0",
        "outputId": "9f30bd5b-8c15-4821-b3c1-db41fc796674"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1731 - val_accuracy: 1.0000 - val_loss: 0.2787 - learning_rate: 5.0000e-04\n",
            "Epoch 2/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.1859 - val_accuracy: 1.0000 - val_loss: 0.2466 - learning_rate: 5.0000e-04\n",
            "Epoch 3/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.1803 - val_accuracy: 1.0000 - val_loss: 0.2341 - learning_rate: 5.0000e-04\n",
            "Epoch 4/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.1801 - val_accuracy: 1.0000 - val_loss: 0.2142 - learning_rate: 5.0000e-04\n",
            "Epoch 5/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1323 - val_accuracy: 1.0000 - val_loss: 0.2053 - learning_rate: 5.0000e-04\n",
            "Epoch 6/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1204 - val_accuracy: 1.0000 - val_loss: 0.1856 - learning_rate: 5.0000e-04\n",
            "Epoch 7/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1097 - val_accuracy: 1.0000 - val_loss: 0.1720 - learning_rate: 5.0000e-04\n",
            "Epoch 8/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1114 - val_accuracy: 1.0000 - val_loss: 0.1581 - learning_rate: 5.0000e-04\n",
            "Epoch 9/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0952 - val_accuracy: 1.0000 - val_loss: 0.1513 - learning_rate: 5.0000e-04\n",
            "Epoch 10/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.1000 - val_accuracy: 1.0000 - val_loss: 0.1401 - learning_rate: 5.0000e-04\n",
            "Epoch 11/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.1026 - val_accuracy: 1.0000 - val_loss: 0.1248 - learning_rate: 5.0000e-04\n",
            "Epoch 12/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0997 - val_accuracy: 1.0000 - val_loss: 0.1108 - learning_rate: 5.0000e-04\n",
            "Epoch 13/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.1163 - val_accuracy: 1.0000 - val_loss: 0.1042 - learning_rate: 5.0000e-04\n",
            "Epoch 14/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9830 - loss: 0.0872 - val_accuracy: 1.0000 - val_loss: 0.0986 - learning_rate: 5.0000e-04\n",
            "Epoch 15/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0739 - val_accuracy: 1.0000 - val_loss: 0.0979 - learning_rate: 5.0000e-04\n",
            "Epoch 16/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0807 - val_accuracy: 1.0000 - val_loss: 0.0939 - learning_rate: 5.0000e-04\n",
            "Epoch 17/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0652 - val_accuracy: 1.0000 - val_loss: 0.0903 - learning_rate: 5.0000e-04\n",
            "Epoch 18/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0594 - val_accuracy: 1.0000 - val_loss: 0.0868 - learning_rate: 5.0000e-04\n",
            "Epoch 19/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0788 - val_accuracy: 1.0000 - val_loss: 0.0854 - learning_rate: 5.0000e-04\n",
            "Epoch 20/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0717 - val_accuracy: 1.0000 - val_loss: 0.0867 - learning_rate: 5.0000e-04\n",
            "Epoch 21/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0615 - val_accuracy: 1.0000 - val_loss: 0.0800 - learning_rate: 5.0000e-04\n",
            "Epoch 22/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0649 - val_accuracy: 1.0000 - val_loss: 0.0749 - learning_rate: 5.0000e-04\n",
            "Epoch 23/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0766 - val_accuracy: 1.0000 - val_loss: 0.0707 - learning_rate: 5.0000e-04\n",
            "Epoch 24/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0828 - val_accuracy: 1.0000 - val_loss: 0.0695 - learning_rate: 5.0000e-04\n",
            "Epoch 25/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0724 - val_accuracy: 1.0000 - val_loss: 0.0695 - learning_rate: 5.0000e-04\n",
            "Epoch 26/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0625 - val_accuracy: 1.0000 - val_loss: 0.0638 - learning_rate: 5.0000e-04\n",
            "Epoch 27/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0585 - val_accuracy: 1.0000 - val_loss: 0.0621 - learning_rate: 5.0000e-04\n",
            "Epoch 28/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0598 - val_accuracy: 1.0000 - val_loss: 0.0606 - learning_rate: 5.0000e-04\n",
            "Epoch 29/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0520 - val_accuracy: 1.0000 - val_loss: 0.0626 - learning_rate: 5.0000e-04\n",
            "Epoch 30/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0582 - val_accuracy: 1.0000 - val_loss: 0.0598 - learning_rate: 5.0000e-04\n",
            "Epoch 31/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0519 - val_accuracy: 1.0000 - val_loss: 0.0592 - learning_rate: 5.0000e-04\n",
            "Epoch 32/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0649 - val_accuracy: 1.0000 - val_loss: 0.0552 - learning_rate: 5.0000e-04\n",
            "Epoch 33/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0531 - val_accuracy: 1.0000 - val_loss: 0.0531 - learning_rate: 5.0000e-04\n",
            "Epoch 34/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0439 - val_accuracy: 1.0000 - val_loss: 0.0526 - learning_rate: 5.0000e-04\n",
            "Epoch 35/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0387 - val_accuracy: 1.0000 - val_loss: 0.0517 - learning_rate: 5.0000e-04\n",
            "Epoch 36/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0702 - val_accuracy: 1.0000 - val_loss: 0.0525 - learning_rate: 5.0000e-04\n",
            "Epoch 37/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0534 - val_accuracy: 1.0000 - val_loss: 0.0526 - learning_rate: 5.0000e-04\n",
            "Epoch 38/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0453 - val_accuracy: 1.0000 - val_loss: 0.0513 - learning_rate: 5.0000e-04\n",
            "Epoch 39/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0469 - val_accuracy: 1.0000 - val_loss: 0.0502 - learning_rate: 5.0000e-04\n",
            "Epoch 40/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9879 - loss: 0.0531 - val_accuracy: 1.0000 - val_loss: 0.0520 - learning_rate: 5.0000e-04\n",
            "Epoch 41/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0400 - val_accuracy: 1.0000 - val_loss: 0.0518 - learning_rate: 5.0000e-04\n",
            "Epoch 42/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 0.0500 - learning_rate: 5.0000e-04\n",
            "Epoch 43/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0404 - val_accuracy: 1.0000 - val_loss: 0.0480 - learning_rate: 5.0000e-04\n",
            "Epoch 44/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 1.0000 - val_loss: 0.0477 - learning_rate: 5.0000e-04\n",
            "Epoch 45/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0421 - val_accuracy: 1.0000 - val_loss: 0.0470 - learning_rate: 5.0000e-04\n",
            "Epoch 46/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0506 - val_accuracy: 1.0000 - val_loss: 0.0461 - learning_rate: 5.0000e-04\n",
            "Epoch 47/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0410 - val_accuracy: 1.0000 - val_loss: 0.0455 - learning_rate: 5.0000e-04\n",
            "Epoch 48/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0356 - val_accuracy: 1.0000 - val_loss: 0.0451 - learning_rate: 5.0000e-04\n",
            "Epoch 49/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0389 - val_accuracy: 1.0000 - val_loss: 0.0442 - learning_rate: 5.0000e-04\n",
            "Epoch 50/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0429 - val_accuracy: 1.0000 - val_loss: 0.0432 - learning_rate: 5.0000e-04\n",
            "Epoch 51/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0393 - val_accuracy: 1.0000 - val_loss: 0.0427 - learning_rate: 5.0000e-04\n",
            "Epoch 52/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0378 - val_accuracy: 1.0000 - val_loss: 0.0427 - learning_rate: 5.0000e-04\n",
            "Epoch 53/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0370 - val_accuracy: 1.0000 - val_loss: 0.0423 - learning_rate: 5.0000e-04\n",
            "Epoch 54/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0382 - val_accuracy: 1.0000 - val_loss: 0.0418 - learning_rate: 5.0000e-04\n",
            "Epoch 55/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0372 - val_accuracy: 1.0000 - val_loss: 0.0411 - learning_rate: 5.0000e-04\n",
            "Epoch 56/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0396 - val_accuracy: 1.0000 - val_loss: 0.0405 - learning_rate: 5.0000e-04\n",
            "Epoch 57/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0432 - val_accuracy: 1.0000 - val_loss: 0.0402 - learning_rate: 5.0000e-04\n",
            "Epoch 58/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0332 - val_accuracy: 1.0000 - val_loss: 0.0399 - learning_rate: 5.0000e-04\n",
            "Epoch 59/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0506 - val_accuracy: 1.0000 - val_loss: 0.0390 - learning_rate: 5.0000e-04\n",
            "Epoch 60/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0450 - val_accuracy: 1.0000 - val_loss: 0.0388 - learning_rate: 5.0000e-04\n",
            "Epoch 61/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0413 - val_accuracy: 1.0000 - val_loss: 0.0384 - learning_rate: 5.0000e-04\n",
            "Epoch 62/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0340 - val_accuracy: 1.0000 - val_loss: 0.0381 - learning_rate: 5.0000e-04\n",
            "Epoch 63/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0327 - val_accuracy: 1.0000 - val_loss: 0.0375 - learning_rate: 5.0000e-04\n",
            "Epoch 64/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0363 - val_accuracy: 1.0000 - val_loss: 0.0371 - learning_rate: 5.0000e-04\n",
            "Epoch 65/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0511 - val_accuracy: 0.9615 - val_loss: 0.1246 - learning_rate: 5.0000e-04\n",
            "Epoch 66/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9384 - loss: 0.1743 - val_accuracy: 0.9231 - val_loss: 0.2966 - learning_rate: 5.0000e-04\n",
            "Epoch 67/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9485 - loss: 0.1653 - val_accuracy: 1.0000 - val_loss: 0.0717 - learning_rate: 5.0000e-04\n",
            "Epoch 68/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9835 - loss: 0.0936 - val_accuracy: 0.9615 - val_loss: 0.1009 - learning_rate: 5.0000e-04\n",
            "Epoch 69/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0712 - val_accuracy: 1.0000 - val_loss: 0.0617 - learning_rate: 5.0000e-04\n",
            "Epoch 70/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9865 - loss: 0.0651 - val_accuracy: 1.0000 - val_loss: 0.0568 - learning_rate: 5.0000e-04\n",
            "Epoch 71/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0602 - val_accuracy: 1.0000 - val_loss: 0.0638 - learning_rate: 5.0000e-04\n",
            "Epoch 72/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0668 - val_accuracy: 1.0000 - val_loss: 0.0627 - learning_rate: 5.0000e-04\n",
            "Epoch 73/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0555 - val_accuracy: 1.0000 - val_loss: 0.0596 - learning_rate: 5.0000e-04\n",
            "Epoch 74/300\n",
            "\u001b[1m21/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9838 - loss: 0.0758\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9849 - loss: 0.0739 - val_accuracy: 1.0000 - val_loss: 0.0628 - learning_rate: 5.0000e-04\n",
            "Epoch 75/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0486 - val_accuracy: 1.0000 - val_loss: 0.0593 - learning_rate: 2.5000e-04\n",
            "Epoch 76/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9948 - loss: 0.0523 - val_accuracy: 1.0000 - val_loss: 0.0575 - learning_rate: 2.5000e-04\n",
            "Epoch 77/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0425 - val_accuracy: 1.0000 - val_loss: 0.0561 - learning_rate: 2.5000e-04\n",
            "Epoch 78/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0397 - val_accuracy: 1.0000 - val_loss: 0.0547 - learning_rate: 2.5000e-04\n",
            "Epoch 79/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0385 - val_accuracy: 1.0000 - val_loss: 0.0544 - learning_rate: 2.5000e-04\n",
            "Epoch 80/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9629 - loss: 0.0774 - val_accuracy: 1.0000 - val_loss: 0.0544 - learning_rate: 2.5000e-04\n",
            "Epoch 81/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0399 - val_accuracy: 1.0000 - val_loss: 0.0539 - learning_rate: 2.5000e-04\n",
            "Epoch 82/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0417 - val_accuracy: 1.0000 - val_loss: 0.0531 - learning_rate: 2.5000e-04\n",
            "Epoch 83/300\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0403 - val_accuracy: 1.0000 - val_loss: 0.0527 - learning_rate: 2.5000e-04\n",
            "Epoch 84/300\n",
            "\u001b[1m23/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0374\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0382 - val_accuracy: 1.0000 - val_loss: 0.0524 - learning_rate: 2.5000e-04\n",
            "\n",
            "VAL ACCURACY LSTM: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_path_lstm = os.path.join(MODELS_DIR, \"sign_model_lstm_v2.keras\")\n",
        "model_lstm.save(keras_path_lstm)\n",
        "print(\"Modelo LSTM V2 guardado en:\", keras_path_lstm)\n",
        "\n",
        "labels_path = os.path.join(MODELS_DIR, \"label_names_v2.json\")\n",
        "with open(labels_path, \"w\") as f:\n",
        "    json.dump(label_names, f)\n",
        "print(\"Listo. Guardado en:\", labels_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7vxqh71xhxn",
        "outputId": "bf88a643-958e-4c6a-8689-ee93056c4a33"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo LSTM V2 guardado en: /content/drive/MyDrive/SignProject/models/sign_model_lstm_v2.keras\n",
            "Listo. Guardado en: /content/drive/MyDrive/SignProject/models/label_names_v2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_full, acc_full = model_lstm.evaluate(X_seq, y_seq, verbose=0)\n",
        "print(f\"Accuracy sobre TODO el dataset (solo referencia): {acc_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWNPPt8Pv0dV",
        "outputId": "78a87a15-520a-4522-d153-b7cd5ef01b41"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy sobre TODO el dataset (solo referencia): 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. FunciÃ³n para probar un video cualquiera (LSTM)**"
      ],
      "metadata": {
        "id": "meQ0V__gwlzS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-M45cqSmKOub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. USO DEL MODELO LSTM CON UN VIDEO\n",
        "\n",
        "\n",
        "def load_lstm_model_and_labels():\n",
        "    model_path = os.path.join(MODELS_DIR, \"sign_model_lstm_v2.keras\")\n",
        "    labels_path = os.path.join(MODELS_DIR, \"label_names_v2.json\")\n",
        "    mean_path = os.path.join(MODELS_DIR, \"feature_mean.npy\")\n",
        "    std_path = os.path.join(MODELS_DIR, \"feature_std.npy\")\n",
        "\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    with open(labels_path, \"r\") as f:\n",
        "        label_names = json.load(f)\n",
        "\n",
        "    feature_mean = np.load(mean_path)\n",
        "    feature_std = np.load(std_path)\n",
        "\n",
        "    return model, label_names, feature_mean, feature_std\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_video_to_sequence(video_path, max_frames=MAX_FRAMES):\n",
        "    \"\"\"\n",
        "    Procesa un video con MediaPipe Holistic y devuelve una secuencia\n",
        "    (1, max_frames, 225) lista para el modelo LSTM.\n",
        "    - Usamos pad_or_truncate para que sea consistente con el entrenamiento.\n",
        "    - NO hay aleatoriedad aquÃ­ (solo center-crop si es muy largo).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frames_feats = []\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=1,\n",
        "        enable_segmentation=False,\n",
        "        refine_face_landmarks=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5,\n",
        "    ) as holis:\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holis.process(frame_rgb)\n",
        "\n",
        "            feats = extract_landmarks_from_results(results)  # (225,)\n",
        "            frames_feats.append(feats)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames_feats) == 0:\n",
        "        # Secuencia vacÃ­a, devolvemos todo ceros\n",
        "        seq = np.zeros((max_frames, N_FEATURES), dtype=np.float32)\n",
        "    else:\n",
        "        seq_full = np.stack(frames_feats, axis=0)  # (T, 225)\n",
        "        seq = pad_or_truncate(seq_full, max_frames=max_frames)\n",
        "\n",
        "    # AÃ±adimos dim de batch: (1, max_frames, 225)\n",
        "    return seq[np.newaxis, ...].astype(np.float32)\n",
        "\n",
        "\n",
        "def predict_video_lstm(video_path, max_frames=MAX_FRAMES):\n",
        "    model, label_names, feature_mean, feature_std = load_lstm_model_and_labels()\n",
        "\n",
        "    # Secuencia sin normalizar\n",
        "    seq = preprocess_video_to_sequence(video_path, max_frames=max_frames)  # (1, T, 225)\n",
        "\n",
        "    # Aplicamos la MISMA normalizaciÃ³n que en entrenamiento\n",
        "    seq_norm = (seq - feature_mean) / feature_std\n",
        "\n",
        "    probs = model.predict(seq_norm, verbose=0)[0]  # (num_classes,)\n",
        "    idx = int(np.argmax(probs))\n",
        "    label = label_names[idx]\n",
        "    conf = float(probs[idx])\n",
        "\n",
        "    print(f\"\\nVideo: {video_path}\")\n",
        "    print(f\"PredicciÃ³n LSTM: {label} (confianza {conf:.2f})\")\n",
        "    print(\"\\nDistribuciÃ³n de probabilidades:\")\n",
        "    for i, name in enumerate(label_names):\n",
        "        print(f\"  {name}: {probs[i]:.2f}\")\n",
        "\n",
        "    return label, conf, probs\n",
        "\n"
      ],
      "metadata": {
        "id": "MCt_awTAwqnb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_video_path = os.path.join(BASE_DIR, \"raw_videos\", \"Disculpa\", \"disculpa.mp4\")\n",
        "predict_video_lstm(test_video_path, max_frames=MAX_FRAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPEh6Y4JwuWl",
        "outputId": "2a709705-fc45-4f52-b59f-27d98f29c3ca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Video: /content/drive/MyDrive/SignProject/raw_videos/Disculpa/disculpa.mp4\n",
            "PredicciÃ³n LSTM: Disculpa (confianza 1.00)\n",
            "\n",
            "DistribuciÃ³n de probabilidades:\n",
            "  Disculpa: 1.00\n",
            "  Inyeccion : 0.00\n",
            "  Marron: 0.00\n",
            "  Por favor: 0.00\n",
            "  Resfriado: 0.00\n",
            "  Tos: 0.00\n",
            "  Verde: 0.00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Disculpa',\n",
              " 0.9982185959815979,\n",
              " array([9.9821860e-01, 1.3293563e-04, 6.7201798e-04, 1.2750592e-04,\n",
              "        2.1198047e-04, 4.0160911e-04, 2.3532695e-04], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate_model_on_raw(RAW_DIR, max_frames=MAX_FRAMES):\n",
        "    \"\"\"\n",
        "    EvalÃºa el modelo LSTM usando todos los videos RAW.\n",
        "    Retorna:\n",
        "      - reporte por clase\n",
        "      - matriz de confusiÃ³n\n",
        "    \"\"\"\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    classes = sorted([c for c in os.listdir(RAW_DIR) if os.path.isdir(os.path.join(RAW_DIR, c))])\n",
        "    print(\"Clases encontradas:\", classes)\n",
        "\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(RAW_DIR, cls)\n",
        "        video_files = [\n",
        "            f for f in os.listdir(class_dir)\n",
        "            if f.lower().endswith((\".mp4\", \".mov\", \".avi\", \".mkv\"))\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nEvaluando clase '{cls}': {len(video_files)} videos\")\n",
        "\n",
        "        for fname in video_files:\n",
        "            video_path = os.path.join(class_dir, fname)\n",
        "            label, conf, _ = predict_video_lstm(video_path, max_frames=max_frames)\n",
        "\n",
        "            true_labels.append(cls)\n",
        "            pred_labels.append(label)\n",
        "\n",
        "            print(f\"  {fname:<25} -> predicciÃ³n: {label:<12} (conf {conf:.2f})\")\n",
        "\n",
        "    # ---------- MÃ‰TRICAS ----------\n",
        "    print(\"\\n===== ACCURACY POR CLASE =====\")\n",
        "    counter_true = Counter(true_labels)\n",
        "    counter_correct = Counter()\n",
        "\n",
        "    for t, p in zip(true_labels, pred_labels):\n",
        "        if t == p:\n",
        "            counter_correct[t] += 1\n",
        "\n",
        "    for cls in classes:\n",
        "        total = counter_true[cls]\n",
        "        correct = counter_correct[cls]\n",
        "        acc = correct / total if total > 0 else 0\n",
        "        print(f\"{cls:<12}: {correct}/{total}  -> {acc*100:.1f}%\")\n",
        "\n",
        "    # Matriz de confusiÃ³n\n",
        "    print(\"\\n===== MATRIZ DE CONFUSIÃ“N =====\")\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=classes)\n",
        "    print(cm)\n",
        "\n",
        "    # Reporte sklearn bonito\n",
        "    print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
        "    print(classification_report(true_labels, pred_labels, target_names=classes))\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "# EJECUTAR\n",
        "RAW_DIR = os.path.join(BASE_DIR, \"raw_videos\")\n",
        "evaluate_model_on_raw(RAW_DIR, max_frames=MAX_FRAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "t8QQs9pg08sP",
        "outputId": "1a13537a-b01b-4afc-80cf-0fddfdb2e846"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases encontradas: ['Disculpa', 'Inyeccion ', 'Marron', 'Por favor', 'Resfriado', 'Tos', 'Verde']\n",
            "\n",
            "Evaluando clase 'Disculpa': 3 videos\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2690190858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# EJECUTAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mRAW_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw_videos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mevaluate_model_on_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_FRAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2690190858.py\u001b[0m in \u001b[0;36mevaluate_model_on_raw\u001b[0;34m(RAW_DIR, max_frames)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvideo_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_video_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4084851899.py\u001b[0m in \u001b[0;36mpredict_video_lstm\u001b[0;34m(video_path, max_frames)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Secuencia sin normalizar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_video_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_frames\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, T, 225)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Aplicamos la MISMA normalizaciÃ³n que en entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4084851899.py\u001b[0m in \u001b[0;36mpreprocess_video_to_sequence\u001b[0;34m(video_path, max_frames)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mframe_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mholis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_landmarks_from_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (225,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/python/solutions/holistic.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \"\"\"\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    338\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;31m# output stream names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}