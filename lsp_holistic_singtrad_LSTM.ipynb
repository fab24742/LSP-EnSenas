{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports + Drive + paths"
      ],
      "metadata": {
        "id": "LIhegBQMltON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.14 opencv-python==4.10.0.84 scikit-learn tensorflowjs --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GsFvhfDsnQfw",
        "outputId": "f486cb98-fa2b-4d74-fb21-d40cfbc33d50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sphinx 8.2.3 requires packaging>=23.0, but you have packaging 20.9 which is incompatible.\n",
            "db-dtypes 1.4.4 requires packaging>=24.2.0, but you have packaging 20.9 which is incompatible.\n",
            "shap 0.50.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "libpysal 4.13.0 requires packaging>=22, but you have packaging 20.9 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "langsmith 0.4.42 requires packaging>=23.2, but you have packaging 20.9 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 20.9 which is incompatible.\n",
            "pandas-gbq 0.30.0 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires packaging>=21, but you have packaging 20.9 which is incompatible.\n",
            "langchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 20.9 which is incompatible.\n",
            "astropy 7.1.1 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "jupyter-server 2.14.0 requires packaging>=22.0, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2025.10.1 requires packaging>=24.1, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFUJ-njylm8t",
        "outputId": "6ec4f0f6-c597-4633-ae8d-98cd6fd0e342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TF version: 2.19.0\n",
            "OpenCV version: 4.12.0\n",
            "RAW_DIR: /content/drive/MyDrive/SignProject/raw_videos\n",
            "PROCESSED_DIR: /content/drive/MyDrive/SignProject/processed\n",
            "MODELS_DIR: /content/drive/MyDrive/SignProject/models\n"
          ]
        }
      ],
      "source": [
        "# 0. IMPORTS BÃSICOS\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "import mediapipe as mp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"OpenCV version:\", cv2.__version__)\n",
        "\n",
        "# 0.1 RUTAS DEL PROYECTO\n",
        "BASE_DIR = \"/content/drive/MyDrive/SignProject\"\n",
        "RAW_DIR = os.path.join(BASE_DIR, \"raw_videos\")   # antes decÃ­a \"raw\"\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, \"processed\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
        "print(\"MODELS_DIR:\", MODELS_DIR)\n",
        "\n",
        "\n",
        "mp_holistic = mp.solutions.holistic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. FunciÃ³n de extracciÃ³n de landmarks (misma lÃ³gica que entrenamiento)"
      ],
      "metadata": {
        "id": "oWlHupjqnfGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_landmarks_from_results(results):\n",
        "    \"\"\"\n",
        "    Convierte los resultados de MediaPipe Holistic en un vector 1D (225,)\n",
        "    con pose (33), mano izq (21) y mano der (21).\n",
        "    Cada punto = (x, y, z) => 75 * 3 = 225 features.\n",
        "    \"\"\"\n",
        "    def get_xyz(landmarks, n_points):\n",
        "        if landmarks is None:\n",
        "            data = [[0.0, 0.0, 0.0]] * n_points\n",
        "        else:\n",
        "            data = [[lm.x, lm.y, lm.z] for lm in landmarks]\n",
        "            if len(data) < n_points:\n",
        "                # Rellenar si faltan\n",
        "                data += [[0.0, 0.0, 0.0]] * (n_points - len(data))\n",
        "            data = data[:n_points]\n",
        "        return data\n",
        "\n",
        "    pose = get_xyz(results.pose_landmarks.landmark if results.pose_landmarks else None, 33)\n",
        "    left_hand = get_xyz(results.left_hand_landmarks.landmark if results.left_hand_landmarks else None, 21)\n",
        "    right_hand = get_xyz(results.right_hand_landmarks.landmark if results.right_hand_landmarks else None, 21)\n",
        "\n",
        "    all_points = pose + left_hand + right_hand\n",
        "    return np.array(all_points, dtype=np.float32).flatten()  # (225,)\n",
        "\n",
        "\n",
        "def mirror_sequence(seq):\n",
        "    \"\"\"\n",
        "    Recibe una secuencia (T, 225) y devuelve una versiÃ³n espejada:\n",
        "      - Refleja en horizontal: x -> 1.0 - x\n",
        "      - Intercambia mano izquierda y mano derecha\n",
        "\n",
        "    Asume el orden:\n",
        "      0-32  -> pose (33 puntos)\n",
        "      33-53 -> mano izquierda (21 puntos)\n",
        "      54-74 -> mano derecha (21 puntos)\n",
        "    \"\"\"\n",
        "    if seq.ndim != 2 or seq.shape[1] != 225:\n",
        "        raise ValueError(f\"mirror_sequence espera (T, 225), recibiÃ³ {seq.shape}\")\n",
        "\n",
        "    T, D = seq.shape\n",
        "    pts = seq.reshape(T, 75, 3).copy()  # (T, 75, 3)\n",
        "\n",
        "    # 1) Espejar en eje X (horizontal)\n",
        "    pts[..., 0] = 1.0 - pts[..., 0]\n",
        "\n",
        "    # 2) Separar segmentos: pose, mano izq, mano der\n",
        "    pose   = pts[:, :33, :]      # (T, 33, 3)\n",
        "    left   = pts[:, 33:54, :]    # (T, 21, 3)\n",
        "    right  = pts[:, 54:75, :]    # (T, 21, 3)\n",
        "\n",
        "    # 3) Reconstruir como: pose + (mano derecha como izquierda) + (mano izquierda como derecha)\n",
        "    pts_mirrored = np.concatenate([pose, right, left], axis=1)  # (T, 75, 3)\n",
        "\n",
        "    # 4) Volver a (T, 225)\n",
        "    return pts_mirrored.reshape(T, 225).astype(np.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "OkhS5Kk-njWU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. ConstrucciÃ³n del dataset secuencial para LSTM**\n",
        "\n",
        "Cada video = 1 muestra\n",
        "Cada muestra = secuencia de max_frames frames, cada frame = 225 features."
      ],
      "metadata": {
        "id": "r3fJx3PynlhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequence_dataset_from_raw(\n",
        "    RAW_DIR,\n",
        "    outfile=\"landmarks_sequences_v1.npz\",\n",
        "    max_frames=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Crea un dataset secuencial:\n",
        "      X_seq -> (n_videos, max_frames, 225)\n",
        "      y_seq -> (n_videos,)\n",
        "    Cada video = 1 muestra (secuencia de hasta max_frames).\n",
        "    Si el video tiene menos frames, se rellena con ceros (padding).\n",
        "    \"\"\"\n",
        "    classes = sorted(os.listdir(RAW_DIR))\n",
        "    classes = [c for c in classes if os.path.isdir(os.path.join(RAW_DIR, c))]\n",
        "    print(\"Clases encontradas:\", classes)\n",
        "\n",
        "    label_map = {cls_name: i for i, cls_name in enumerate(classes)}\n",
        "    print(\"label_map:\", label_map)\n",
        "\n",
        "    X_sequences = []\n",
        "    y_sequences = []\n",
        "    label_names = classes  # para guardar luego\n",
        "\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=1,\n",
        "        enable_segmentation=False,\n",
        "        refine_face_landmarks=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    ) as holistic:\n",
        "\n",
        "        for cls_name in classes:\n",
        "            class_dir = os.path.join(RAW_DIR, cls_name)\n",
        "            videos = [f for f in os.listdir(class_dir)\n",
        "                      if f.lower().endswith((\".mp4\", \".mov\", \".avi\", \".mkv\"))]\n",
        "\n",
        "            print(f\"\\nProcesando clase '{cls_name}' ({len(videos)} videos)...\")\n",
        "\n",
        "            for vid_name in videos:\n",
        "                video_path = os.path.join(class_dir, vid_name)\n",
        "                cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "                frames_feats = []\n",
        "\n",
        "                while True:\n",
        "                    ret, frame = cap.read()\n",
        "                    if not ret:\n",
        "                        break\n",
        "\n",
        "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    results = holistic.process(frame_rgb)\n",
        "\n",
        "                    vec = extract_landmarks_from_results(results)  # (225,)\n",
        "                    frames_feats.append(vec)\n",
        "\n",
        "                    # Si ya tenemos max_frames, paramos\n",
        "                    if len(frames_feats) >= max_frames:\n",
        "                        break\n",
        "\n",
        "                cap.release()\n",
        "\n",
        "                if len(frames_feats) == 0:\n",
        "                    print(f\"âš ï¸ Video sin frames vÃ¡lidos: {video_path}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "                                # Convertir a array (n_frames, 225)\n",
        "                seq = np.array(frames_feats, dtype=np.float32)\n",
        "\n",
        "                # Pad o recortar a max_frames\n",
        "                if seq.shape[0] < max_frames:\n",
        "                    pad_len = max_frames - seq.shape[0]\n",
        "                    pad = np.zeros((pad_len, seq.shape[1]), dtype=np.float32)\n",
        "                    seq = np.concatenate([seq, pad], axis=0)\n",
        "                else:\n",
        "                    seq = seq[:max_frames, :]\n",
        "\n",
        "                # ================================\n",
        "                #  ğŸ‘‡  AQUI DUPLICAMOS CON ESPEJO\n",
        "                # ================================\n",
        "\n",
        "                # Secuencia original\n",
        "                X_sequences.append(seq)               # (max_frames, 225)\n",
        "                y_sequences.append(label_map[cls_name])\n",
        "\n",
        "                # Secuencia espejada (cambia mano dominante)\n",
        "                try:\n",
        "                    seq_mirror = mirror_sequence(seq)     # (max_frames, 225)\n",
        "                    X_sequences.append(seq_mirror)\n",
        "                    y_sequences.append(label_map[cls_name])\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ Error espejando video {video_path}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X_seq = np.stack(X_sequences, axis=0)      # (n_videos, max_frames, 225)\n",
        "    y_seq = np.array(y_sequences, dtype=np.int32)\n",
        "\n",
        "    print(\"\\n=== RESUMEN SECUENCIAL ===\")\n",
        "    print(\"X_seq shape:\", X_seq.shape)  # (n_videos, max_frames, 225)\n",
        "    print(\"y_seq shape:\", y_seq.shape)\n",
        "\n",
        "    out_path = os.path.join(PROCESSED_DIR, outfile)\n",
        "    np.savez(out_path, X_seq=X_seq, y_seq=y_seq, label_names=np.array(label_names))\n",
        "    print(\"Guardado en:\", out_path)\n",
        "\n",
        "    return X_seq, y_seq, label_names\n",
        "\n",
        "\n",
        "# EJECUTAR UNA VEZ PARA CREAR DATASET\n",
        "MAX_FRAMES = 20  # puedes cambiarlo, pero mantÃ©n el mismo valor en todo el notebook\n",
        "\n",
        "X_seq, y_seq, label_names = build_sequence_dataset_from_raw(\n",
        "    RAW_DIR,\n",
        "    outfile=\"landmarks_sequences_v1.npz\",\n",
        "    max_frames=MAX_FRAMES\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFEOUzrbnn0U",
        "outputId": "8e485921-7d73-44d2-d521-a321b6cfc333"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases encontradas: ['Azul', 'Crema', 'Marron', 'Negro', 'Verde']\n",
            "label_map: {'Azul': 0, 'Crema': 1, 'Marron': 2, 'Negro': 3, 'Verde': 4}\n",
            "\n",
            "Procesando clase 'Azul' (3 videos)...\n",
            "\n",
            "Procesando clase 'Crema' (3 videos)...\n",
            "\n",
            "Procesando clase 'Marron' (3 videos)...\n",
            "\n",
            "Procesando clase 'Negro' (3 videos)...\n",
            "\n",
            "Procesando clase 'Verde' (3 videos)...\n",
            "\n",
            "=== RESUMEN SECUENCIAL ===\n",
            "X_seq shape: (30, 20, 225)\n",
            "y_seq shape: (30,)\n",
            "Guardado en: /content/drive/MyDrive/SignProject/processed/landmarks_sequences_v1.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Carga de dataset secuencial + split por porcentaje"
      ],
      "metadata": {
        "id": "9nTkIboupUbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. CARGA DE DATASET SECUENCIAL Y SPLIT ESTRATIFICADO\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "data_path = os.path.join(PROCESSED_DIR, \"landmarks_sequences_v1.npz\")\n",
        "data = np.load(data_path, allow_pickle=True)\n",
        "\n",
        "# OJO: las claves que guardamos en la parte 2 son estas:\n",
        "#   X_seq -> (n_videos, max_frames, 225)\n",
        "#   y_seq -> (n_videos,)\n",
        "#   label_names -> lista de nombres de clase\n",
        "X_seq = data[\"X_seq\"]\n",
        "y_seq = data[\"y_seq\"]              # ya son enteros 0,1,2,...\n",
        "label_names = data[\"label_names\"].tolist()\n",
        "\n",
        "print(\"X_seq:\", X_seq.shape)\n",
        "print(\"y_seq:\", y_seq.shape)\n",
        "print(\"Clases:\", label_names)\n",
        "\n",
        "# ---- Split por porcentaje pero robusto ----\n",
        "n_samples = X_seq.shape[0]\n",
        "classes_unique = np.unique(y_seq)\n",
        "n_classes = len(classes_unique)\n",
        "\n",
        "val_ratio = 0.2  # ~20% validaciÃ³n\n",
        "\n",
        "# n_val aproximado\n",
        "n_val = int(round(n_samples * val_ratio))\n",
        "\n",
        "# asegurar al menos 1 muestra por clase en VALIDACIÃ“N\n",
        "n_val = max(n_val, n_classes)\n",
        "\n",
        "# asegurar al menos 1 muestra por clase en TRAIN\n",
        "n_val = min(n_val, n_samples - n_classes)\n",
        "\n",
        "print(f\"Total muestras: {n_samples}, clases: {n_classes}, val: {n_val}\")\n",
        "\n",
        "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
        "    X_seq,\n",
        "    y_seq,\n",
        "    test_size=n_val,   # entero: sklearn usa exactamente ese nÃºmero\n",
        "    stratify=y_seq,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"Train seq:\", X_train_seq.shape, y_train_seq.shape)\n",
        "print(\"Val seq:\",   X_val_seq.shape,   y_val_seq.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s5aW7Ret5mu",
        "outputId": "d3f3c0d1-235a-4e87-c456-ad5d546d1c3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq: (30, 20, 225)\n",
            "y_seq: (30,)\n",
            "Clases: ['Azul', 'Crema', 'Marron', 'Negro', 'Verde']\n",
            "Total muestras: 30, clases: 5, val: 6\n",
            "Train seq: (24, 20, 225) (24,)\n",
            "Val seq: (6, 20, 225) (6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Modelo LSTM (temporal)"
      ],
      "metadata": {
        "id": "8yLjfnFvvX07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. MODELO LSTM TEMPORAL\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "num_classes = len(label_names)\n",
        "max_frames = X_seq.shape[1]\n",
        "n_features = X_seq.shape[2]\n",
        "\n",
        "model_lstm = keras.Sequential([\n",
        "    layers.Input(shape=(max_frames, n_features)),   # (T, 225)\n",
        "\n",
        "    # Ignora frames de padding (todo ceros)\n",
        "    layers.Masking(mask_value=0.0),\n",
        "\n",
        "    layers.LSTM(128, return_sequences=False),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_lstm.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "xHvXPlLhvcYZ",
        "outputId": "086a3c4c-6024-4011-d5e3-6de14a5e2ef6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ masking_1 (\u001b[38;5;33mMasking\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m225\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m181,248\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              â”‚           \u001b[38;5;34m325\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ masking_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">181,248</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m189,829\u001b[0m (741.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">189,829</span> (741.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m189,829\u001b[0m (741.52 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">189,829</span> (741.52 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 ENTRENAMIENTO\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=20,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_lstm = model_lstm.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    validation_data=(X_val_seq, y_val_seq),\n",
        "    epochs=450,\n",
        "    batch_size=4,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "val_loss, val_acc = model_lstm.evaluate(X_val_seq, y_val_seq, verbose=0)\n",
        "print(\"\\nVAL ACCURACY LSTM:\", val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cVa2Jt8vts0",
        "outputId": "21ba2f6b-df9b-480b-8ddc-158759477f6e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 0.5089 - loss: 1.4660 - val_accuracy: 0.0000e+00 - val_loss: 1.7954\n",
            "Epoch 2/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.3893 - loss: 1.4056 - val_accuracy: 0.0000e+00 - val_loss: 1.8332\n",
            "Epoch 3/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2244 - loss: 1.5628 - val_accuracy: 0.1667 - val_loss: 1.8641\n",
            "Epoch 4/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3327 - loss: 1.4099 - val_accuracy: 0.1667 - val_loss: 1.8512\n",
            "Epoch 5/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.5190 - loss: 1.2626 - val_accuracy: 0.3333 - val_loss: 1.7743\n",
            "Epoch 6/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5137 - loss: 1.2635 - val_accuracy: 0.3333 - val_loss: 1.7850\n",
            "Epoch 7/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6673 - loss: 1.2457 - val_accuracy: 0.3333 - val_loss: 1.8105\n",
            "Epoch 8/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6958 - loss: 1.0576 - val_accuracy: 0.3333 - val_loss: 1.7942\n",
            "Epoch 9/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6494 - loss: 0.9913 - val_accuracy: 0.3333 - val_loss: 1.6946\n",
            "Epoch 10/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7077 - loss: 1.0377 - val_accuracy: 0.3333 - val_loss: 1.6439\n",
            "Epoch 11/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7202 - loss: 1.0985 - val_accuracy: 0.3333 - val_loss: 1.7645\n",
            "Epoch 12/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8137 - loss: 0.7709 - val_accuracy: 0.3333 - val_loss: 1.9149\n",
            "Epoch 13/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8667 - loss: 0.6908 - val_accuracy: 0.3333 - val_loss: 1.9658\n",
            "Epoch 14/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7220 - loss: 0.7106 - val_accuracy: 0.3333 - val_loss: 2.1174\n",
            "Epoch 15/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7429 - loss: 0.6935 - val_accuracy: 0.1667 - val_loss: 2.1113\n",
            "Epoch 16/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7304 - loss: 0.6887 - val_accuracy: 0.3333 - val_loss: 1.8905\n",
            "Epoch 17/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7268 - loss: 0.6541 - val_accuracy: 0.1667 - val_loss: 2.0549\n",
            "Epoch 18/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7845 - loss: 0.7169 - val_accuracy: 0.3333 - val_loss: 2.2307\n",
            "Epoch 19/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6381 - loss: 0.7831 - val_accuracy: 0.1667 - val_loss: 1.8276\n",
            "Epoch 20/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6054 - loss: 0.8140 - val_accuracy: 0.1667 - val_loss: 1.9420\n",
            "Epoch 21/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7786 - loss: 0.6068 - val_accuracy: 0.3333 - val_loss: 1.9613\n",
            "Epoch 22/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6935 - loss: 0.6811 - val_accuracy: 0.3333 - val_loss: 2.2428\n",
            "Epoch 23/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7024 - loss: 0.9460 - val_accuracy: 0.0000e+00 - val_loss: 2.1881\n",
            "Epoch 24/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6494 - loss: 0.8411 - val_accuracy: 0.3333 - val_loss: 2.2308\n",
            "Epoch 25/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6476 - loss: 0.8154 - val_accuracy: 0.3333 - val_loss: 2.2384\n",
            "Epoch 26/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7470 - loss: 0.6382 - val_accuracy: 0.1667 - val_loss: 2.0317\n",
            "Epoch 27/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7899 - loss: 0.5783 - val_accuracy: 0.3333 - val_loss: 2.0361\n",
            "Epoch 28/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7899 - loss: 0.4666 - val_accuracy: 0.3333 - val_loss: 2.3653\n",
            "Epoch 29/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9083 - loss: 0.3936 - val_accuracy: 0.3333 - val_loss: 2.0812\n",
            "Epoch 30/450\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9423 - loss: 0.2589 - val_accuracy: 0.3333 - val_loss: 2.0338\n",
            "\n",
            "VAL ACCURACY LSTM: 0.3333333432674408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_path_lstm = os.path.join(MODELS_DIR, \"sign_model_lstm_v1.keras\")\n",
        "model_lstm.save(keras_path_lstm)\n",
        "print(\"Modelo LSTM guardado en:\", keras_path_lstm)\n",
        "\n",
        "labels_path = os.path.join(MODELS_DIR, \"label_names.json\")\n",
        "\n",
        "with open(labels_path, \"w\") as f:\n",
        "    json.dump(label_names, f)\n",
        "\n",
        "print(\"Listo. Guardado en:\", labels_path)\n",
        "print(label_names)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7vxqh71xhxn",
        "outputId": "09570413-683e-4153-ab88-5ad3f4d92c44"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo LSTM guardado en: /content/drive/MyDrive/SignProject/models/sign_model_lstm_v1.keras\n",
            "Listo. Guardado en: /content/drive/MyDrive/SignProject/models/label_names.json\n",
            "['Azul', 'Crema', 'Marron', 'Negro', 'Verde']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_full, acc_full = model_lstm.evaluate(X_seq, y_seq, verbose=0)\n",
        "print(f\"Accuracy sobre TODO el dataset (solo referencia): {acc_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWNPPt8Pv0dV",
        "outputId": "22d5b27d-9c1d-455d-f65b-861bed2a5639"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy sobre TODO el dataset (solo referencia): 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. FunciÃ³n para probar un video cualquiera (LSTM)**"
      ],
      "metadata": {
        "id": "meQ0V__gwlzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. USO DEL MODELO LSTM CON UN VIDEO\n",
        "\n",
        "\n",
        "\n",
        "def load_lstm_model_and_labels():\n",
        "    model_path = os.path.join(MODELS_DIR, \"sign_model_lstm_v1.keras\")\n",
        "    labels_path = os.path.join(MODELS_DIR, \"label_names.json\")\n",
        "\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "    with open(labels_path, \"r\") as f:\n",
        "        label_names = json.load(f)\n",
        "\n",
        "    return model, label_names\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_video_to_sequence(video_path, max_frames=20):\n",
        "    \"\"\"\n",
        "    Procesa un video con MediaPipe Holistic y devuelve una secuencia\n",
        "    (1, max_frames, 225) lista para el modelo LSTM.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frames_feats = []\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        model_complexity=1,\n",
        "        enable_segmentation=False,\n",
        "        refine_face_landmarks=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    ) as holistic:\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = holistic.process(frame_rgb)\n",
        "\n",
        "            vec = extract_landmarks_from_results(results)  # (225,)\n",
        "            frames_feats.append(vec)\n",
        "\n",
        "            if len(frames_feats) >= max_frames:\n",
        "                break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames_feats) == 0:\n",
        "        raise ValueError(f\"Video sin frames vÃ¡lidos: {video_path}\")\n",
        "\n",
        "    seq = np.array(frames_feats, dtype=np.float32)\n",
        "\n",
        "    # Pad o recortar\n",
        "    if seq.shape[0] < max_frames:\n",
        "        pad_len = max_frames - seq.shape[0]\n",
        "        pad = np.zeros((pad_len, seq.shape[1]), dtype=np.float32)\n",
        "        seq = np.concatenate([seq, pad], axis=0)\n",
        "    else:\n",
        "        seq = seq[:max_frames, :]\n",
        "\n",
        "    seq = seq.reshape(1, max_frames, seq.shape[1])  # (1, T, 225)\n",
        "    return seq\n",
        "\n",
        "\n",
        "def predict_video_lstm(video_path, max_frames=20):\n",
        "    model, label_names = load_lstm_model_and_labels()\n",
        "    seq = preprocess_video_to_sequence(video_path, max_frames=max_frames)\n",
        "\n",
        "    probs = model.predict(seq, verbose=0)[0]  # (num_classes,)\n",
        "    idx = int(np.argmax(probs))\n",
        "    label = label_names[idx]\n",
        "    conf = float(probs[idx])\n",
        "\n",
        "    print(f\"\\nVideo: {video_path}\")\n",
        "    print(f\"PredicciÃ³n LSTM: {label} (confianza {conf:.2f})\")\n",
        "    print(\"\\nDistribuciÃ³n de probabilidades:\")\n",
        "    for i, name in enumerate(label_names):\n",
        "        print(f\"  {name}: {probs[i]:.2f}\")\n",
        "\n",
        "    return label, conf, probs\n"
      ],
      "metadata": {
        "id": "MCt_awTAwqnb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_video_path = os.path.join(BASE_DIR, \"raw_videos\", \"Marron\", \"Marronf.mp4\")\n",
        "predict_video_lstm(test_video_path, max_frames=MAX_FRAMES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPEh6Y4JwuWl",
        "outputId": "30fb8e11-18eb-4c58-db32-7530d9d5c0f3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Video: /content/drive/MyDrive/SignProject/raw_videos/Marron/Marronf.mp4\n",
            "PredicciÃ³n LSTM: Marron (confianza 0.48)\n",
            "\n",
            "DistribuciÃ³n de probabilidades:\n",
            "  Azul: 0.07\n",
            "  Crema: 0.30\n",
            "  Marron: 0.48\n",
            "  Negro: 0.07\n",
            "  Verde: 0.08\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Marron',\n",
              " 0.48444125056266785,\n",
              " array([0.06586322, 0.29921758, 0.48444125, 0.06920952, 0.08126838],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}